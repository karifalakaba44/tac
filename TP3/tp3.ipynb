{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering de documents de la décénie 1960–1969"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import string\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/txt/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Décénie 1960–1969"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DECADE = '1960'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in sorted(os.listdir(data_path)) if f\"_{DECADE[:-1]}\" in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de fichiers\n",
    "files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [open(data_path + f, \"r\", encoding=\"utf-8\").read() for f in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de textes\n",
    "texts[0][:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une fonction de pré-traitement\n",
    "def preprocessing(text, stem=True):\n",
    "    \"\"\" Tokenize text and remove punctuation \"\"\"\n",
    "    text = text.translate(string.punctuation)\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=preprocessing,\n",
    "    stop_words=stopwords.words('french'),\n",
    "    max_df=0.5,\n",
    "    min_df=0.1,\n",
    "    lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectors = vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Détail de la matrice\n",
    "tfidf_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(\n",
    "    tfidf_vectors[0].toarray()[0],\n",
    "    index=vectorizer.get_feature_names_out()\n",
    "    ).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_array = tfidf_vectors.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nombre de clusters définis à 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLUSTERS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_model = KMeans(n_clusters=N_CLUSTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = km_model.fit_predict(tfidf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = collections.defaultdict(list)\n",
    "\n",
    "for idx, label in enumerate(clusters):\n",
    "    clustering[label].append(files[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(dict(clustering))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "reduced_vectors = pca.fit_transform(tfidf_vectors.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_vectors[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = reduced_vectors[:, 0]\n",
    "y_axis = reduced_vectors[:, 1]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "scatter = plt.scatter(x_axis, y_axis, s=100, c=clusters)\n",
    "\n",
    "# Ajouter les centroïdes\n",
    "centroids = pca.transform(km_model.cluster_centers_)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],  marker = \"x\", s=100, linewidths = 2, color='black')\n",
    "\n",
    "# Ajouter la légende\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=set(clusters), title=\"Clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction des documents de chaque cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Récupération des étiquettes de cluster pour chaque document\n",
    "labels = km_model.labels_\n",
    "\n",
    "# Créer un dictionnaire pour stocker les documents de chaque cluster\n",
    "clusters_documents = {}\n",
    "for i, label in enumerate(labels):\n",
    "    if label not in clusters_documents:\n",
    "        clusters_documents[label] = []\n",
    "    clusters_documents[label].append(i)  # Ajouter l'index du document au cluster correspondant\n",
    "\n",
    "for cluster_label, documents_indices in clusters_documents.items():\n",
    "    print(f\"Cluster {cluster_label}:\")\n",
    "    for doc_index in documents_indices:\n",
    "        print(texts[doc_index])  # Afficher le document correspondant à l'indice\n",
    "    print(\"\\n\")  # Ajouter un saut de ligne entre les clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse : mots clés des clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yake\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantier l'extracteur de mots clés\n",
    "kw_extractor = yake.KeywordExtractor(lan=\"fr\", top=50)\n",
    "kw_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupérer les mots clés pour chaque document de chaque clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parcours chaque cluster\n",
    "for cluster_label, documents_indices in clusters_documents.items():\n",
    "    print(f\"Mots-clés pour le cluster {cluster_label}:\")\n",
    "    for doc_index in documents_indices:\n",
    "        text = texts[doc_index]  # Récupérer le texte du document\n",
    "        keywords = kw_extractor.extract_keywords(text)  # Extraire les mots-clés du texte\n",
    "        kept = []\n",
    "        for kw, score in keywords:\n",
    "            words = kw.split()\n",
    "            if len(words) == 2:\n",
    "                kept.append(kw)\n",
    "        print(f\"Document {doc_index} mentions these keywords: {', '.join(kept)}...\")\n",
    "    print(\"\\n\")  # Saut de ligne entre les clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regroupement des mots clés les plus fréquents par clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Charger les mots vides de la langue correspondante \n",
    "stop_words = set(stopwords.words(\"french\"))\n",
    "\n",
    "# Ajouter des mots à la liste des stop words\n",
    "additional_stop_words = [\"les\", \"plus\", \"cette\", \"fait\", \"faire\", \"être\", \"deux\", \"comme\", \"dont\", \"tout\",\n",
    "                        \"ils\", \"elles\", \"il\", \"elle\", \"bien\", \"sans\", \"peut\", \"tous\", \"après\", \"ainsi\", \"donc\", \"cet\", \"sous\",\n",
    "                        \"celle\", \"entre\", \"encore\", \"toutes\", \"pendant\", \"moins\", \"dire\", \"cela\", \"non\",\n",
    "                        \"faut\", \"trois\", \"aussi\", \"dit\", \"avoir\", \"doit\", \"contre\", \"depuis\", \"autres\",\n",
    "                        \"van\", \"het\", \"autre\", \"jusqu\", \"ville\", \"rossel\", \"dem\", \"si\", \"car\", \"autant\",\n",
    "                        \"toute\", \"très\", \"bien\", \"aucun\", \"comme\", \"celui\", \"chaque\", \"plusieurs\",\n",
    "                        \"toutes\", \"trop\", \"aucune\", \"parce\", \"quelques\", \"quel\", \"quelle\", \"quels\",\n",
    "                        \"quelles\", \"lorsqu\", \"lorsque\", \"dès\", \"dans\", \"leurs\", \"peu\", \"toute\", \"toutes\",\n",
    "                        \"DÈS\", \"près\", \"quart\", \"part\", \"ETC\", \"demi\", \"bas\", \"grand\", \"demande\", \"vers\",\n",
    "                        \"avant\", \"vers\",\"aussi\",\"ans\",\"leurs\",\"très\"]\n",
    "\n",
    "# Parcours chaque cluster\n",
    "for cluster_label, documents_indices in clusters_documents.items():\n",
    "    print(f\"Mots-clés fréquents pour le cluster {cluster_label}:\")\n",
    "    \n",
    "    # Collecter tous les mots non vides, non numériques et ne contenant pas une seule lettre de tous les documents du cluster\n",
    "    all_words = []\n",
    "    for doc_index in documents_indices:\n",
    "        text = texts[doc_index]\n",
    "        words = re.findall(r'\\b[A-Za-zÀ-ÿ]+\\b', text)  # Trouver les mots alphabétiques\n",
    "        # Filtrer les mots vides, les mots qui sont des chiffres et les mots d'une seule lettre\n",
    "        words = [word for word in words if word.lower() not in stop_words and not word.isdigit() and len(word) > 1]\n",
    "        all_words.extend(words)  # Ajouter les mots satisfaisant les critères à la liste\n",
    "    \n",
    "    # Calculer la fréquence des mots non vides, non numériques et ne contenant pas une seule lettre\n",
    "    word_freq = Counter(all_words)\n",
    "    \n",
    "    # Sélectionner les mots les plus fréquents\n",
    "    top_keywords = word_freq.most_common(200)\n",
    "    \n",
    "    # Afficher les mots-clés fréquents pour ce cluster\n",
    "    for keyword, frequency in top_keywords:\n",
    "        print(f\"Mot-clé: {keyword}, Fréquence: {frequency}\")\n",
    "    print(\"\\n\")  # Saut de ligne entre les clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse : nuage de mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parcours chaque cluster\n",
    "for cluster_label, documents_indices in clusters_documents.items():\n",
    "    print(f\"Nuage de mots pour le cluster {cluster_label}:\")\n",
    "    \n",
    "    \n",
    "    # Créer une chaîne de mots pour le nuage de mots\n",
    "    wordcloud_text = ' '.join([keyword for keyword, _ in top_keywords])\n",
    "    \n",
    "    # Créer et afficher le nuage de mots\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(wordcloud_text)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Nuage de mots pour le cluster {cluster_label}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement d'un modèle word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    \"\"\"Tokenize and Lemmatize sentences\"\"\"\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in open(self.filename, encoding='utf-8', errors=\"backslashreplace\"):\n",
    "            yield [unidecode(w.lower()) for w in wordpunct_tokenize(line)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = f\"../data/sents.txt\"\n",
    "sentences = MySentences(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phrases = Phrases(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(bigram_phrases.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bigram_phrases.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_ = list(bigram_phrases.vocab.keys())[144]\n",
    "print(key_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phrases.vocab[key_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phraser = Phraser(phrases_model=bigram_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_phrases = Phrases(bigram_phraser[sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_phraser = Phraser(phrases_model=trigram_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(trigram_phraser[bigram_phraser[sentences]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptation des paramètres window et min_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = Word2Vec(\n",
    "    corpus, # On passe le corpus de ngrams que nous venons de créer\n",
    "    vector_size=32, # Le nombre de dimensions dans lesquelles le contexte des mots devra être réduit, aka. vector_size\n",
    "    window=10, # La taille du \"contexte\", ici 5 mots avant et après le mot observé\n",
    "    min_count=10, # On ignore les mots qui n'apparaissent pas au moins 5 fois dans le corpus\n",
    "    workers=4, # Permet de paralléliser l'entraînement du modèle en 4 threads\n",
    "    epochs=5 # Nombre d'itérations du réseau de neurones sur le jeu de données pour ajuster les paramètres avec la descente de gradient, aka. epochs.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = f\"../data/newspapers.model\"\n",
    "model.save(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"../data/newspapers.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv[\"paris\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculer la similarité entre deux termes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity(\"paris\", \"capitale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chercher les mots les plus proches d'un terme donné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"paris\", topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar(positive=['paris', 'londres'], negative=['belgique']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tac_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
